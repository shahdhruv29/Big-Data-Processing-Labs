{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqXztiRbCXsIgQdDwRH5WG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahdhruv29/Big-Data-Processing-Labs/blob/main/labs/Lab4_Spark_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 04 - Programming Spark SQL**\n",
        "\n",
        "# **Dhruv Shah - 202103017**"
      ],
      "metadata": {
        "id": "3GWdQAVjJEHC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmBUmp-8I8Zy",
        "outputId": "87305aa7-dcd0-4310-c3cd-47e2e7ad08ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=c322569e0d6e4e26d3a03fbf1459897f711704692c3993f8a6cf61c680c324a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klKU9SfGJnBb",
        "outputId": "c51d86ab-9d43-4248-9450-09182d442157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, desc, col\n",
        "spark = SparkSession.builder.appName(\"US-Sales\").getOrCreate()"
      ],
      "metadata": {
        "id": "zYZJaKb_KH5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_folder_path = \"/content/gdrive/My Drive/datasets/us-sales/orders.csv\"\n",
        "\n",
        "# Read all CSV files in the folder into a DataFrame\n",
        "df_orders = spark.read.option(\"header\", \"true\").csv(csv_folder_path)\n",
        "df_stores = spark.read.option(\"header\", \"true\").csv(\"/content/gdrive/My Drive/datasets/us-sales/stores.csv\")\n",
        "df_customers = spark.read.option(\"header\", \"true\").csv(\"/content/gdrive/My Drive/datasets/us-sales/customers.csv\")\n",
        "df_products = spark.read.option(\"header\", \"true\").csv(\"/content/gdrive/My Drive/datasets/us-sales/products.csv\")\n",
        "df_regions = spark.read.option(\"header\", \"true\").csv(\"/content/gdrive/My Drive/datasets/us-sales/regions.csv\")\n",
        "df_sales_teams = spark.read.option(\"header\", \"true\").csv(\"/content/gdrive/My Drive/datasets/us-sales/sales_teams.csv\")"
      ],
      "metadata": {
        "id": "8S_UZEdgKItt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Compute top-10 selling products in terms of numbers (i. e. sum(qty))"
      ],
      "metadata": {
        "id": "ShlvKF4fMtv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, desc\n",
        "top_selling_products = df_orders.groupBy(\"ProductID\").agg(sum(\"OrderQuantity\").alias(\"TotalQuantity\")).orderBy(desc(\"TotalQuantity\")).limit(10)\n",
        "\n",
        "print(\"Top 10 Selling Products (by Total Quantity Sold):\")\n",
        "top_selling_products.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8vJUqnHMxB3",
        "outputId": "62406f4e-d99c-41b7-ab6c-2cdb6fb1deb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Selling Products (by Total Quantity Sold):\n",
            "+---------+-------------+\n",
            "|ProductID|TotalQuantity|\n",
            "+---------+-------------+\n",
            "|       23|        956.0|\n",
            "|       37|        896.0|\n",
            "|        8|        879.0|\n",
            "|        4|        878.0|\n",
            "|       40|        855.0|\n",
            "|       41|        854.0|\n",
            "|       22|        837.0|\n",
            "|       38|        832.0|\n",
            "|       27|        830.0|\n",
            "|       12|        827.0|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Compute top-10 selling products in terms of value (i. e. sum (qty*price))"
      ],
      "metadata": {
        "id": "YCwf3VIbM67J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_selling_products_by_value = df_orders.groupBy(\"ProductID\").agg(sum(df_orders[\"OrderQuantity\"] * df_orders[\"UnitPrice\"]).alias(\"TotalValue\")) \\\n",
        "    .orderBy(desc(\"TotalValue\")).limit(10)\n",
        "\n",
        "print(\"Top 10 Selling Products (by Total Value Sold):\")\n",
        "top_selling_products_by_value.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZgHQEDHM2Zo",
        "outputId": "7e36d17f-3884-40a9-faf8-1835cf1a7f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Selling Products (by Total Value Sold):\n",
            "+---------+------------------+\n",
            "|ProductID|        TotalValue|\n",
            "+---------+------------------+\n",
            "|       23| 2358788.599999999|\n",
            "|       40|         2130841.2|\n",
            "|        4|2071546.1999999993|\n",
            "|       37|         2052886.7|\n",
            "|       41|         2049958.8|\n",
            "|        5|2011333.3000000012|\n",
            "|        2|         2005638.3|\n",
            "|       35| 1981973.900000001|\n",
            "|        8|1976895.2999999996|\n",
            "|       17|1925111.0000000002|\n",
            "+---------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Compute top-10 profit making products. Profit = sum(qty*(price-cost))"
      ],
      "metadata": {
        "id": "iOS8CHYyNOwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_profit_making_products = df_orders.groupBy(\"ProductID\").agg(sum(df_orders[\"OrderQuantity\"] * (df_orders[\"UnitPrice\"] - df_orders[\"UnitCost\"])).alias(\"TotalProfit\")) \\\n",
        "    .orderBy(desc(\"TotalProfit\")).limit(10)\n",
        "\n",
        "print(\"Top 10 Profit-Making Products:\")\n",
        "top_profit_making_products.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH-9qMn-NKQF",
        "outputId": "014110cd-14bd-4fb8-b251-a36313c014ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Profit-Making Products:\n",
            "+---------+-----------------+\n",
            "|ProductID|      TotalProfit|\n",
            "+---------+-----------------+\n",
            "|       23|908818.7699999997|\n",
            "|        8|796037.5299999998|\n",
            "|        4|786277.2900000003|\n",
            "|        2|783599.7399999995|\n",
            "|       40|767278.9100000005|\n",
            "|       41|761318.8800000004|\n",
            "|        5|750981.4400000005|\n",
            "|       37|743189.7299999997|\n",
            "|       35|741447.8800000004|\n",
            "|       11|741098.0599999997|\n",
            "+---------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Give top-3 stores selling product product number 25"
      ],
      "metadata": {
        "id": "aHWid0i0NieO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter orders for product number 25\n",
        "product_25_orders = df_orders.filter(df_orders[\"ProductID\"] == 25)\n",
        "\n",
        "# Group by store and sum the quantities sold\n",
        "top_stores_product_25 = product_25_orders.groupBy(\"StoreID\").agg(sum(\"OrderQuantity\").alias(\"TotalQuantity\")).orderBy(desc(\"TotalQuantity\")).limit(3)\n",
        "\n",
        "print(\"Top 3 Stores Selling Product 25:\")\n",
        "top_stores_product_25.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHNJXlHHNYz8",
        "outputId": "3db7c0f1-4264-48d5-8b2c-a474aed082c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Stores Selling Product 25:\n",
            "+-------+-------------+\n",
            "|StoreID|TotalQuantity|\n",
            "+-------+-------------+\n",
            "|     56|         16.0|\n",
            "|     26|         14.0|\n",
            "|    350|         13.0|\n",
            "+-------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Give top-3 products sold in midwest region"
      ],
      "metadata": {
        "id": "QcwXixt9Nxmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table1 = df_orders.join(df_products, df_orders[\"ProductID\"] == df_products[\"ProductID\"]).drop(df_orders[\"ProductID\"])\n",
        "table2 = table1.join(df_stores, table1[\"StoreID\"] == df_stores[\"_StoreID\"]).drop(df_stores[\"_StoreID\"])\n",
        "table3 = table2.join(df_regions, table2[\"StateCode\"] == df_regions[\"StateCode\"]).drop(df_regions[\"StateCode\"]).where(col(\"Region\") == \"Midwest\")\n",
        "\n",
        "# Group by Region, ProductID, and ProductName and sum the quantities sold\n",
        "top_products_midwest = table3.groupBy(\"Region\", \"ProductID\", \"ProductName\").agg(sum(\"OrderQuantity\").alias(\"Total Quantity\")).orderBy(desc(\"Total Quantity\")).limit(3)\n",
        "\n",
        "print(\"Top 3 Products Sold in Midwest Region:\")\n",
        "top_products_midwest.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0nn47cfNqgp",
        "outputId": "b375277f-d813-4a32-e142-3e9cd1c8f4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Products Sold in Midwest Region:\n",
            "+-------+---------+------------+--------------+\n",
            "| Region|ProductID| ProductName|Total Quantity|\n",
            "+-------+---------+------------+--------------+\n",
            "|Midwest|       25|TV and video|         224.0|\n",
            "|Midwest|       29|    Pendants|         206.0|\n",
            "|Midwest|       23| Accessories|         206.0|\n",
            "+-------+---------+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Give region wise quantity sold for each product. Compute: Region, Product ID, Sum(Qty). Region is related to a order through sales team."
      ],
      "metadata": {
        "id": "3noUWlLnOwT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join df_orders with df_stores and df_sales_teams\n",
        "table1 = df_orders.join(df_stores, df_orders[\"StoreID\"] == df_stores[\"_StoreID\"]).join(df_sales_teams, \"SalesTeamID\", \"inner\")\n",
        "\n",
        "# Group by region and product, and sum the quantities sold\n",
        "region_product_quantity = table1.groupBy(\"Region\", \"ProductID\").agg(sum(\"OrderQuantity\").alias(\"TotalQuantity\"))\n",
        "\n",
        "print(\"Region-wise Quantity Sold for Each Product:\")\n",
        "region_product_quantity.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cLLzbfzN5nB",
        "outputId": "ca214663-307d-4792-f373-d606b1216778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Region-wise Quantity Sold for Each Product:\n",
            "+---------+---------+-------------+\n",
            "|   Region|ProductID|TotalQuantity|\n",
            "+---------+---------+-------------+\n",
            "|    South|       10|        203.0|\n",
            "|  Midwest|       20|        224.0|\n",
            "|     West|       15|        192.0|\n",
            "|    South|       18|        144.0|\n",
            "|  Midwest|       28|        209.0|\n",
            "|     West|        9|        262.0|\n",
            "|  Midwest|       45|        264.0|\n",
            "|  Midwest|        1|        192.0|\n",
            "|  Midwest|        7|        231.0|\n",
            "|     West|        1|        158.0|\n",
            "|    South|        6|         71.0|\n",
            "|  Midwest|       25|        246.0|\n",
            "|Northeast|       42|        162.0|\n",
            "|    South|       40|        165.0|\n",
            "|     West|       13|        259.0|\n",
            "|    South|       44|        127.0|\n",
            "|  Midwest|       36|        179.0|\n",
            "|Northeast|       33|        190.0|\n",
            "|  Midwest|       22|        247.0|\n",
            "|  Midwest|        2|        212.0|\n",
            "+---------+---------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Compute Average monthly sale in terms of numbers at each store; that , that is on average what numbers of a product are sold on a store in a month."
      ],
      "metadata": {
        "id": "EkoKj1PvO1Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, month,avg\n",
        "\n",
        "# Convert OrderDate to a date type\n",
        "df1 = df_orders.withColumn(\"OrderDate\", df_orders[\"OrderDate\"].cast(\"date\"))\n",
        "\n",
        "# Extract year and month from OrderDate\n",
        "df1 = df_orders.withColumn(\"Year\", year(\"OrderDate\")).withColumn(\"Month\", month(\"OrderDate\"))\n",
        "\n",
        "# Group by store, year, and month, and compute the average quantity sold\n",
        "average_monthly_sale = df1.groupBy(\"StoreID\", \"Year\", \"Month\").agg(avg(\"OrderQuantity\").alias(\"AverageQuantity\"))\n",
        "\n",
        "print(\"Average Monthly Sale at Each Store:\")\n",
        "average_monthly_sale.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuhCyRlgPCth",
        "outputId": "02cf5636-524a-4522-bdca-bd9548e34383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Monthly Sale at Each Store:\n",
            "+-------+----+-----+-----------------+\n",
            "|StoreID|Year|Month|  AverageQuantity|\n",
            "+-------+----+-----+-----------------+\n",
            "|    231|2018|    6|              3.5|\n",
            "|    358|2018|    7|              7.0|\n",
            "|    360|2018|    7|              5.0|\n",
            "|    286|2018|    9|              5.0|\n",
            "|    330|2018|   10|              7.0|\n",
            "|    262|2018|   11|              2.0|\n",
            "|    147|2019|    2|              4.0|\n",
            "|    172|2019|    2|              6.0|\n",
            "|    202|2019|    3|              2.0|\n",
            "|     72|2019|    4|              7.0|\n",
            "|     84|2019|    4|              3.0|\n",
            "|    320|2019|    5|              4.0|\n",
            "|    151|2019|    6|              1.0|\n",
            "|    344|2019|    6|              1.0|\n",
            "|    161|2019|    8|              2.0|\n",
            "|     60|2019|    8|              7.0|\n",
            "|     38|2019|   11|              3.0|\n",
            "|     41|2019|   12|              3.5|\n",
            "|     30|2020|    1|              6.0|\n",
            "|     84|2020|    2|4.333333333333333|\n",
            "+-------+----+-----+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Compute sales bifurcation of each warehouse; that total sales amount through each channel"
      ],
      "metadata": {
        "id": "t7SXQLsjPkuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "# Group by WarehouseCode and Sales_Channel and calculate the sales amount\n",
        "sales_bifurcation = df_orders.groupBy(\"WarehouseCode\", \"Sales_Channel\") \\\n",
        "    .agg(sum(col(\"OrderQuantity\") * col(\"UnitPrice\")).alias(\"Sales Amount\")) \\\n",
        "    .orderBy(col(\"Sales Amount\"))\n",
        "\n",
        "print(\"Sales Bifurcation of Each Warehouse:\")\n",
        "sales_bifurcation.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ed810C3PIt3",
        "outputId": "2587165f-286a-4a0a-abe3-10a84ff79973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales Bifurcation of Each Warehouse:\n",
            "+-------------+-------------+------------------+\n",
            "|WarehouseCode|Sales_Channel|      Sales Amount|\n",
            "+-------------+-------------+------------------+\n",
            "| WARE-NBV1002|    Wholesale| 703781.3999999998|\n",
            "| WARE-MKL1006|    Wholesale|          837071.2|\n",
            "| WARE-NBV1002|  Distributor|1248377.5000000002|\n",
            "| WARE-XYS1001|    Wholesale|1301287.4000000006|\n",
            "| WARE-UHY1004|    Wholesale|1501536.9999999998|\n",
            "| WARE-PUJ1005|    Wholesale|1713940.4000000004|\n",
            "| WARE-MKL1006|  Distributor|1738784.0000000002|\n",
            "| WARE-XYS1001|  Distributor|2099110.0000000005|\n",
            "| WARE-NBV1002|       Online| 2103324.299999999|\n",
            "| WARE-UHY1004|  Distributor|         2269785.8|\n",
            "| WARE-MKL1006|       Online|         2606648.4|\n",
            "| WARE-NBV1002|     In-Store|2856270.3000000003|\n",
            "| WARE-PUJ1005|  Distributor|2998256.6999999993|\n",
            "| WARE-NMK1003|    Wholesale| 3155331.499999998|\n",
            "| WARE-MKL1006|     In-Store|3554832.4000000004|\n",
            "| WARE-XYS1001|       Online| 3827207.500000005|\n",
            "| WARE-UHY1004|       Online| 4093545.900000002|\n",
            "| WARE-PUJ1005|       Online| 4231646.300000002|\n",
            "| WARE-NMK1003|  Distributor| 4455593.800000001|\n",
            "| WARE-XYS1001|     In-Store| 5346941.699999996|\n",
            "+-------------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Compute average \"product retention period\" (i. e. the difference between procurement date and order date) at each warehouse"
      ],
      "metadata": {
        "id": "xoAbYUaJPtdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many rows have NULL values in OrderDate or ProcuredDate\n",
        "null_rows_count = df_orders.filter(col(\"OrderDate\").isNull() | col(\"ProcuredDate\").isNull()).count()\n",
        "print(f\"Number of rows with NULL values in OrderDate or ProcuredDate: {null_rows_count}\")\n",
        "\n",
        "# Show a few examples of the rows with NULL values\n",
        "df_orders.filter(col(\"OrderDate\").isNull() | col(\"ProcuredDate\").isNull()).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl_ZB5DwbXSg",
        "outputId": "4a60f8d1-f833-49c0-cb53-84a5a1964372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows with NULL values in OrderDate or ProcuredDate: 7991\n",
            "+-----------+-------------+-------------+------------+----------+----------+------------+------------+-----------+----------+-------+---------+-------------+---------------+---------+--------+\n",
            "|OrderNumber|Sales_Channel|WarehouseCode|ProcuredDate| OrderDate|  ShipDate|DeliveryDate|CurrencyCode|SalesTeamID|CustomerID|StoreID|ProductID|OrderQuantity|DiscountApplied|UnitPrice|UnitCost|\n",
            "+-----------+-------------+-------------+------------+----------+----------+------------+------------+-----------+----------+-------+---------+-------------+---------------+---------+--------+\n",
            "|SO - 000101|     In-Store| WARE-UHY1004|        NULL|2018-05-31|14-06-2018|  19-06-2018|         USD|          6|        15|    259|       12|            5|          0.075|   1963.1| 1001.18|\n",
            "|SO - 000102|       Online| WARE-NMK1003|        NULL|2018-05-31|22-06-2018|  02-07-2018|         USD|         14|        20|    196|       27|            3|          0.075|   3939.6| 3348.66|\n",
            "|SO - 000103|  Distributor| WARE-UHY1004|        NULL|2018-05-31|21-06-2018|  01-07-2018|         USD|         21|        16|    213|       16|            1|           0.05|   1775.5|  781.22|\n",
            "|SO - 000104|    Wholesale| WARE-NMK1003|        NULL|2018-05-31|02-06-2018|  07-06-2018|         USD|         28|        48|    107|       23|            8|          0.075|   2324.9| 1464.69|\n",
            "|SO - 000105|  Distributor| WARE-NMK1003|        NULL|2018-05-31|16-06-2018|  26-06-2018|         USD|         22|        49|    111|       26|            8|            0.1|   1822.4| 1476.14|\n",
            "|SO - 000106|       Online| WARE-PUJ1005|        NULL|2018-05-31|08-06-2018|  13-06-2018|         USD|         12|        21|    285|        1|            5|           0.05|   1038.5|  446.56|\n",
            "|SO - 000107|     In-Store| WARE-XYS1001|        NULL|2018-05-31|08-06-2018|  14-06-2018|         USD|         10|        14|      6|        5|            4|           0.15|   1192.6|  536.67|\n",
            "|SO - 000108|     In-Store| WARE-PUJ1005|        NULL|2018-05-31|26-06-2018|  01-07-2018|         USD|          6|         9|    280|       46|            5|           0.05|   1815.7| 1525.19|\n",
            "|SO - 000109|     In-Store| WARE-PUJ1005|        NULL|2018-06-01|16-06-2018|  21-06-2018|         USD|          4|         9|    299|       47|            4|            0.3|   3879.3|  2211.2|\n",
            "|SO - 000110|     In-Store| WARE-UHY1004|        NULL|2018-06-01|29-06-2018|  01-07-2018|         USD|         10|        33|    261|       13|            8|           0.05|   1956.4| 1212.97|\n",
            "|SO - 000111|  Distributor| WARE-XYS1001|        NULL|2018-06-01|15-06-2018|  20-06-2018|         USD|         23|        21|     17|       38|            6|            0.1|      201|  124.62|\n",
            "|SO - 000112|     In-Store| WARE-NMK1003|        NULL|2018-06-01|07-06-2018|  17-06-2018|         USD|         10|        21|    152|       40|            5|           0.15|   6277.9| 2762.28|\n",
            "|SO - 000113|     In-Store| WARE-PUJ1005|        NULL|2018-06-01|22-06-2018|  02-07-2018|         USD|          4|        36|    317|       39|            5|           0.05|   1051.9|  641.66|\n",
            "|SO - 000114|     In-Store| WARE-PUJ1005|        NULL|2018-06-01|07-06-2018|  15-06-2018|         USD|          8|        17|    291|       32|            6|           0.15|    254.6|  216.41|\n",
            "|SO - 000115|     In-Store| WARE-NMK1003|        NULL|2018-06-01|15-06-2018|  20-06-2018|         USD|          9|        32|    138|        6|            6|           0.15|   3932.9| 3146.32|\n",
            "|SO - 000116|     In-Store| WARE-MKL1006|        NULL|2018-06-01|24-06-2018|  02-07-2018|         USD|          5|        11|    354|       25|            3|           0.05|   1112.2|  700.69|\n",
            "|SO - 000117|     In-Store| WARE-PUJ1005|        NULL|2018-06-01|19-06-2018|  27-06-2018|         USD|          9|        10|    320|        6|            3|          0.075|   1239.5|  904.84|\n",
            "|SO - 000118|     In-Store| WARE-XYS1001|        NULL|2018-06-01|06-06-2018|  14-06-2018|         USD|          8|        30|     21|        3|            4|            0.1|    984.9|  393.96|\n",
            "|SO - 000119|     In-Store| WARE-MKL1006|        NULL|2018-06-01|07-06-2018|  15-06-2018|         USD|          5|         5|    349|       20|            4|            0.1|   5581.1| 4130.01|\n",
            "|SO - 000120|       Online| WARE-NMK1003|        NULL|2018-06-01|11-06-2018|  17-06-2018|         USD|         14|        23|    134|       24|            4|           0.05|   3095.4| 1795.33|\n",
            "+-----------+-------------+-------------+------------+----------+----------+------------+------------+-----------+----------+-------+---------+-------------+---------------+---------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import datediff, avg\n",
        "\n",
        "# # Calculate the difference between OrderDate and ProcuredDate\n",
        "# date_difference = df_orders.select(\n",
        "#     col(\"WarehouseCode\"),\n",
        "#     datediff(col(\"OrderDate\"), col(\"ProcuredDate\")).alias(\"Date_difference\")\n",
        "# )\n",
        "\n",
        "# # Group by WarehouseCode and calculate the average product retention period\n",
        "# average_retention_period = date_difference.groupBy(\"WarehouseCode\") \\\n",
        "#     .agg(avg('Date_difference').alias(\"product retention period\")) \\\n",
        "#     .orderBy(col(\"product retention period\").asc())\n",
        "\n",
        "# print(\"Average Product Retention Period at Each Warehouse:\")\n",
        "# average_retention_period.show()\n",
        "\n",
        "from pyspark.sql.functions import col, date_format, substring, to_date\n",
        "\n",
        "\n",
        "date_difference = df_orders.select(\n",
        "    col(\"WarehouseCode\"),\n",
        "    ((date_format(\"OrderDate\", \"yyyy\").cast(\"int\") * 365) +\n",
        "     (date_format(\"OrderDate\", \"MM\").cast(\"int\") * 12) +\n",
        "     (date_format(\"OrderDate\", \"dd\").cast(\"int\")) -\n",
        "     (substring(\"ProcuredDate\", -4, 4).cast(\"int\") * 365) -\n",
        "     (substring(\"ProcuredDate\", -7, 2).cast(\"int\") * 12) -\n",
        "     (substring(\"ProcuredDate\", 0, 2).cast(\"int\"))\n",
        "    ).alias(\"avg_diff_in_days\")\n",
        ")\n",
        "\n",
        "# Group by WarehouseCode and calculate the average product retention period\n",
        "average_retention_period = date_difference.groupBy(\"WarehouseCode\") \\\n",
        "    .agg(avg(\"avg_diff_in_days\").alias(\"product_retention_period\")) \\\n",
        "    .orderBy(col(\"product_retention_period\").asc())\n",
        "\n",
        "print(\"Average Product Retention Period at Each Warehouse:\")\n",
        "average_retention_period.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWpI6JlfPq1f",
        "outputId": "4af5787e-d55b-425c-f526-9bfa161162b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Product Retention Period at Each Warehouse:\n",
            "+-------------+------------------------+\n",
            "|WarehouseCode|product_retention_period|\n",
            "+-------------+------------------------+\n",
            "| WARE-XYS1001|                    NULL|\n",
            "| WARE-PUJ1005|                    NULL|\n",
            "| WARE-MKL1006|                    NULL|\n",
            "| WARE-NMK1003|                    NULL|\n",
            "| WARE-UHY1004|                    NULL|\n",
            "| WARE-NBV1002|                    NULL|\n",
            "+-------------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Give Year-Month sales of all products. Here you actually print 'Year-Month', ProductID, sum(qty). Use Order Date for extracting Year and Month of sale. For simplicity you can read order date as string only in YYYY-MM-DD format, and extract the required info accordingly."
      ],
      "metadata": {
        "id": "xkRSteTZQIDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, month, sum\n",
        "\n",
        "# Extract year and month from OrderDate\n",
        "year_month_sale = df_orders.withColumn(\"Year\", year(\"OrderDate\")).withColumn(\"Month\", month(\"OrderDate\"))\n",
        "\n",
        "# Group by Year, Month, and ProductID, and calculate the total quantity sold\n",
        "year_month_sale = year_month_sale.groupBy(\"Year\", \"Month\", \"ProductID\") \\\n",
        "    .agg(sum(col(\"OrderQuantity\")).alias(\"Total Quantity\"))\n",
        "\n",
        "print(\"Year-Month Sale of All Products:\")\n",
        "year_month_sale.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ_f1ypwQMyh",
        "outputId": "1a1e96b5-1056-4370-a06f-e764a6b01c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Year-Month Sale of All Products:\n",
            "+----+-----+---------+--------------+\n",
            "|Year|Month|ProductID|Total Quantity|\n",
            "+----+-----+---------+--------------+\n",
            "|2018|    8|       45|          11.0|\n",
            "|2018|   12|        1|          30.0|\n",
            "|2019|    5|       40|          53.0|\n",
            "|2019|   10|        4|          12.0|\n",
            "|2020|    1|       10|          28.0|\n",
            "|2018|   10|       25|          33.0|\n",
            "|2019|    3|       30|          24.0|\n",
            "|2020|    6|       28|           9.0|\n",
            "|2020|    6|       32|          34.0|\n",
            "|2019|   12|       16|          22.0|\n",
            "|2020|   11|       43|          39.0|\n",
            "|2019|    1|       23|          22.0|\n",
            "|2019|    9|       42|          23.0|\n",
            "|2020|    9|        5|          18.0|\n",
            "|2018|   10|       13|          12.0|\n",
            "|2019|   10|       19|          41.0|\n",
            "|2018|   12|        3|          12.0|\n",
            "|2019|    3|        1|          19.0|\n",
            "|2019|    3|        4|          18.0|\n",
            "|2019|    4|       33|          38.0|\n",
            "+----+-----+---------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. Compute a fact file with the dimensions of \"store_id\", \"product_id\", and \"month_year\". Let facts to be computed are \"quantity\" and \"amount\". Let month_year be represented as YYYY-MM."
      ],
      "metadata": {
        "id": "FTrxKATuV5F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate quantity and amount for each combination of StoreID, ProductID, and OrderDate\n",
        "fact_file = df_orders.cube(\"StoreID\", \"ProductID\", \"OrderDate\") \\\n",
        "    .agg(sum(col(\"OrderQuantity\")).alias(\"Quantity\"), sum(col(\"OrderQuantity\") * col(\"UnitPrice\")).alias(\"Amount\")) \\\n",
        "    .sort(\"StoreID\", \"ProductID\", \"OrderDate\")\n",
        "\n",
        "print(\"Fact File with Dimensions of StoreID, ProductID, and Month-Year (YYYY-MM):\")\n",
        "fact_file.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NUWNpnUQm3t",
        "outputId": "99423b80-f705-456d-e331-5f1c33ece5d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fact File with Dimensions of StoreID, ProductID, and Month-Year (YYYY-MM):\n",
            "+-------+---------+----------+--------+-------------------+\n",
            "|StoreID|ProductID| OrderDate|Quantity|             Amount|\n",
            "+-------+---------+----------+--------+-------------------+\n",
            "|   NULL|     NULL|      NULL| 36162.0|8.269272660000001E7|\n",
            "|   NULL|     NULL|2018-05-31|    39.0|  75629.59999999999|\n",
            "|   NULL|     NULL|2018-06-01|    62.0|           148961.1|\n",
            "|   NULL|     NULL|2018-06-02|    35.0|            79696.5|\n",
            "|   NULL|     NULL|2018-06-03|    59.0| 214125.30000000002|\n",
            "|   NULL|     NULL|2018-06-04|    26.0| 100392.80000000002|\n",
            "|   NULL|     NULL|2018-06-05|    32.0|  89203.79999999999|\n",
            "|   NULL|     NULL|2018-06-06|    20.0|            45332.2|\n",
            "|   NULL|     NULL|2018-06-07|    40.0| 112593.50000000001|\n",
            "|   NULL|     NULL|2018-06-08|    36.0| 39014.100000000006|\n",
            "|   NULL|     NULL|2018-06-09|    37.0|  64420.50000000001|\n",
            "|   NULL|     NULL|2018-06-10|    19.0| 52414.100000000006|\n",
            "|   NULL|     NULL|2018-06-11|    27.0|            56635.1|\n",
            "|   NULL|     NULL|2018-06-12|    51.0| 104781.29999999999|\n",
            "|   NULL|     NULL|2018-06-13|    39.0|  80802.00000000001|\n",
            "|   NULL|     NULL|2018-06-14|    49.0|           105638.9|\n",
            "|   NULL|     NULL|2018-06-15|    51.0|            94985.9|\n",
            "|   NULL|     NULL|2018-06-16|    26.0|            31342.6|\n",
            "|   NULL|     NULL|2018-06-17|    39.0|  74745.19999999998|\n",
            "|   NULL|     NULL|2018-06-18|    41.0|            86671.2|\n",
            "+-------+---------+----------+--------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}